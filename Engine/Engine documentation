Structure:
Engine file/class


Data Loader file/class:
Aggregated data (contains data for aggregated metrics) for monitoring from asc-clickstream-emr-output/trendlines/p2p/agg/data_version=dataset576_V11/ 
for P2P and asc-clickstream-emr-output/trendlines/pf/agg/data_version=dataset576_V11/ for PF is loaded and stored in Spark DataFrame
Cubed data for monitoring from asc-clickstream-emr-output/trendlines/p2p/cube/data_version=dataset576_V11/ for P2P
and asc-clickstream-emr-output/trendlines/pf/cube/data_version=dataset576_V11/ for PF is loaded and stored in Spyrk DataFrame
Result data 

*****************

Metrics file/class
Metrics file/class contain methods for all metrics + method for creating list od all these methods. Anomaly detection engine then iterates
through this list and executes method by method, executing modeling via them, and creating result file used for creation of report later then.

Every function in this file is dedicated to one metric. The reason of creating one method for each metric, despite the code repeats itself,
is primarily because of organization, readability, easier adding, deteting, or updating metrics, because if I refactored code and created
shared functions for some metrics, there would be also a lot of exceptions because of metrics that measure different counts than events, e.g.,
bot_panelists, events_by_products_not_null_pid, etc.

Each function works the same way:
1/ The metric's data is selected form dataset where the metric's data is situated (cube or aggregated data; some metrics appears in both, 
so I took them from cube dataset since it is more comprehensive) by grouping granularity parameters of a metric and date and summing 
the analyzed counts â€“ the sum is required as we have metrics for "All" datasets or "All" domains and we need the overall count for
all that datasets, domains, etc.
2/ The list of unique combinations of metrics is created, e.g., number_of_events per Costco, per Walmart, ...
3/ We iterate through the list of unique metrics, select only data related to the particular metric, i.e., particular combination of
domain, dataset, etc., sort this dataset by date so we have a time series.
4/ Parameters for ARIMA model are selected from result_data dataframe for the particular metric, e.g., particular combination of domain,
dataset, etc.
5/ Time series and parameters are used for calling model function in Model class that returns current day value, expected value for current
day found by model, verdict of decision making process (is current value within the threshold of expected value, is current value missing?),
and parameters of model.
6/ The outputs of model function are used for creating result report for current day that is then stored to S3. Report is then build upon this
report and the parameters for ARIMA model for the particular metric from this report are used the following day for again

****************

Model file/class:
Model file/class contains all statistical tests and methods for pre-processing time series before their modeling as well as models
and modeling algorithms themselves. The main method called from metrics functions is model method that calls statistical tests
and required pre-processing fuctions based on the result of the statistical tests. The ARIMA model's parameters (p, d, q) for a particular
time series, time series for a particular combination of metric, domain, dataset, behavior, patternId, etc., retrieved from Result DataFrame
is passed to modeling function (arima_model func.) that return predicted value and confidence interval. It also returns model's parameters
which are same as the ones passed to the function. On a specified day on which the retarining of the models is required, the hyndman_khandakarr
method is called instead of arima_model method. Hyndman_khandakar method performs Hyndman&Khandakar algorithms for finding new model. It then
return expected value based on time series, confidence interval, and new model's parameters that are stored to Results and used until the
next retraining of models.

